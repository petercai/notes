ä¸‹é¢ä¸ºä½ ç³»ç»Ÿæ€§ä»‹ç» **Python `multiprocessing.Pool` çš„åŸç†ã€ä½¿ç”¨æŠ€å·§ä¸æœ€ä½³å®è·µ**ã€‚å†…å®¹åŒ…æ‹¬æ ¸å¿ƒæœºåˆ¶ã€å¸¸è§é™·é˜±ã€æ€§èƒ½ä¼˜åŒ–ã€å…¸å‹æ¨¡å¼ç­‰ï¼Œé€‚ç”¨äº Python 3.8â€“3.12ã€‚

---

# ğŸ”§ ä¸€ã€`Pool` åŸç†ï¼ˆå·¥ä½œæœºåˆ¶ï¼‰

`multiprocessing.Pool` æ˜¯ä¸€ä¸ª **å¤šè¿›ç¨‹å·¥ä½œæ± **ï¼Œç”¨äºåœ¨å¤šä¸ª worker è¿›ç¨‹ä¸­å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ã€‚

æ ¸å¿ƒæœºåˆ¶ï¼š

## 1. **é¢„å…ˆåˆ›å»ºå›ºå®šæ•°é‡çš„ worker è¿›ç¨‹**

```python
from multiprocessing import Pool

pool = Pool(processes=4)
```

ä¼šç«‹å³åˆ›å»º 4 ä¸ªå­è¿›ç¨‹ï¼Œåç»­ä»»åŠ¡é€šè¿‡ IPC å‘é€åˆ°è¿™äº›è¿›ç¨‹æ‰§è¡Œã€‚

---

## 2. **ä»»åŠ¡è¢«åºåˆ—åŒ–ï¼ˆpickleï¼‰åä¼ è¾“ç»™å­è¿›ç¨‹**

ä¸»è¿›ç¨‹é€šè¿‡ `multiprocessing.Pipe/Queue` å°†ï¼š

- å‡½æ•°ï¼ˆå¿…é¡»å¯ pickleï¼‰
    
- å‚æ•°ï¼ˆå¿…é¡»å¯ pickleï¼‰
    

åºåˆ—åŒ–åä¼ ç»™ workerã€‚

å­è¿›ç¨‹æ‰§è¡Œå®Œåï¼Œå†å°†ç»“æœ pickle å›ä¸»è¿›ç¨‹ã€‚

âš  **ä»»ä½•æ— æ³•åºåˆ—åŒ–çš„ä¸œè¥¿æ— æ³•ä½œä¸ºä»»åŠ¡å‚æ•°ä¼ å…¥ï¼**  
ä¾‹å¦‚ï¼šæ‰“å¼€çš„æ–‡ä»¶å¥æŸ„ã€é”å¯¹è±¡ã€æ•°æ®åº“è¿æ¥ç­‰ã€‚

---

## 3. **worker è¿›ç¨‹é•¿æœŸå­˜æ´»ï¼Œé¿å…åå¤ fork å¼€é”€**

è¿™æ˜¯ Pool çš„ä¸»è¦æ€§èƒ½ä¼˜åŠ¿ï¼š  
åå¤åˆ›å»ºè¿›ç¨‹ = è´µï¼ŒPool è®© worker å¤ç”¨ã€‚

---

## 4. **ä»»åŠ¡è°ƒåº¦ç­–ç•¥**

`Pool` ä¼šæŒ‰ â€œ**ç©ºé—² worker** â†’ åˆ†é…ä¸‹ä¸€ä¸ªä»»åŠ¡â€ çš„æ–¹å¼è¿è¡Œã€‚

é»˜è®¤çš„ä»»åŠ¡åˆ†å‘ç­–ç•¥æ˜¯ **éå…¬å¹³è°ƒåº¦**ï¼Œå› æ­¤ï¼š

- ä»»åŠ¡é•¿çŸ­å·®å¼‚å¤§ â†’ å¯èƒ½é€ æˆ worker ä¸å‡è¡¡
    
- å»ºè®®åˆ†æ‰¹ï¼ˆchunksï¼‰æé«˜æ€§èƒ½ï¼ˆåé¢ä¼šè®²ï¼‰
    

---

# âœ¨ äºŒã€Pool çš„å¸¸ç”¨ API ä¸ç”¨æ³•

## 1. ğŸ”¹ `map()`ï¼ˆæœ€å¸¸ç”¨ï¼‰

é€‚åˆï¼šå¤§é‡å‚æ•°åˆ—è¡¨ â†’ å¯¹åŒä¸€å‡½æ•°æ‰§è¡Œ

```python
results = pool.map(f, data_list)
```

- è¿”å›ç»“æœé¡ºåº = è¾“å…¥é¡ºåº
    
- é˜»å¡ç›´åˆ°æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    

---

## 2. ğŸ”¹ `imap() / imap_unordered()`

é€‚åˆï¼šéœ€è¦â€œè¾¹è®¡ç®—è¾¹å¤„ç†ç»“æœâ€

```python
for r in pool.imap(f, data_list):
    ...
```

- ç±»ä¼¼ç”Ÿæˆå™¨
    
- `imap_unordered` ä¼šæœ€å¿«çš„ä»»åŠ¡å…ˆè¿”å›ï¼Œé€‚ç”¨äº IO/CPU ä¸å‡åŒ€ä»»åŠ¡
    

---

## 3. ğŸ”¹ `apply()` / `apply_async()`

é€‚åˆæ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆåƒæ™®é€šå‡½æ•°ï¼‰

```python
pool.apply(f, args)
pool.apply_async(f, args)
```

`apply_async` å¯ä»¥ç”¨ callbackï¼š

```python
pool.apply_async(f, args, callback=print)
```

---

## 4. ğŸ”¹ å…³é—­ä¸å›æ”¶

```python
pool.close()   # ä¸å†æ¥æ”¶æ–°ä»»åŠ¡
pool.join()    # ç­‰å¾…æ‰€æœ‰ worker ç»“æŸ
```

âš  å¿…é¡»è°ƒç”¨ï¼Œå¦åˆ™è¿›ç¨‹ä¸ä¼šé€€å‡ºã€‚

---

# ğŸ§  ä¸‰ã€ä½¿ç”¨æŠ€å·§ï¼ˆè¿›é˜¶ï¼‰

## 1. **å¿…é¡»æ”¾åœ¨ `if __name__ == "__main__":` ä¸‹ï¼ˆWindows å¿…é¡»ï¼‰**

```python
if __name__ == "__main__":
    pool = Pool()
    ...
```

å¦åˆ™ Windows/macOS spawn æ¨¡å¼ä¼šæ— é™é€’å½’åˆ›å»ºè¿›ç¨‹ã€‚

---

## 2. **ç”¨ `chunksize` æå‡ååé‡**

é»˜è®¤ `chunksize=1` å¯¼è‡´ worker é¢‘ç¹ä¼ è¾“æ•°æ®ï¼ˆå·¨å¤§ CPU å¼€é”€ï¼‰ã€‚

å»ºè®®ï¼š

```python
pool.map(f, data_list, chunksize=100)
```

ç»éªŒå…¬å¼ï¼š

```
chunksize = len(data_list) // (processes * 4)
```

---

## 3. **ä½¿ç”¨ `starmap` è¿›è¡Œå¤šå‚æ•°å‡½æ•°è°ƒç”¨**

```python
def f(a, b):
    ...

pool.starmap(f, [(1, 2), (3, 4)])
```

ä¸ç”¨è‡ªå·±å†™ wrapperã€‚

---

## 4. **é¿å…ä¼ é€’å¤§å¯¹è±¡ï¼šç”¨ Manager / shared_memory**

ä¸è¦è¿™æ ·ï¼š

```python
# BAD
big_data = load_big_data()
pool.map(process, [big_data] * 8)
```

è¿™ä¼šå¤šæ¬¡ pickle å¤§å¯¹è±¡ â†’ å·¨æ…¢

æ¨èï¼š

- `multiprocessing.Manager()`
    
- `shared_memory`
    
- `readonly global variable`ï¼ˆæ¯ä¸ª worker fork åæœ¬åœ°è½½å…¥ä¸€æ¬¡ï¼‰
    

---

## 5. **ä½¿ç”¨ `initializer` åˆå§‹åŒ–æ¯ä¸ª worker**

é€‚åˆå»ºç«‹æ•°æ®åº“è¿æ¥ã€åŠ è½½æ¨¡å‹ã€åŠ è½½å¤§æ–‡ä»¶ã€‚

```python
def init_worker():
    global model
    model = load_model()

Pool(4, initializer=init_worker)
```

è¿™æ ·æ¯ä¸ª worker è‡ªå·±åŠ è½½èµ„æºï¼Œé¿å… pickle å’Œé‡å¤ä¼ è¾“ã€‚

---

# ğŸ å››ã€æœ€ä½³å®è·µï¼ˆå¿…å­¦å¥—è·¯ï¼‰

## â­ 1. å¤§é‡ CPU å¯†é›†ä»»åŠ¡ â†’ å¿…é¡»ç”¨å¤šè¿›ç¨‹ï¼Œè€Œä¸æ˜¯ ThreadPool

CPU bound:

- æ•°å­¦è®¡ç®—
    
- å‹ç¼©
    
- åŠ å¯†
    
- å›¾ç‰‡ã€è§†é¢‘å¤„ç†
    

Python threads è¢« GIL é™åˆ¶ â†’ å¤šè¿›ç¨‹æ˜¯å”¯ä¸€æ­£ç¡®åšæ³•ã€‚

---

## â­ 2. ä»»åŠ¡æ—¶é—´ä¸å‡è¡¡ â†’ ä½¿ç”¨ `imap_unordered`

```python
for r in pool.imap_unordered(f, data_list):
    handle(r)
```

æœ€å…ˆå®Œæˆçš„ä»»åŠ¡å…ˆè¿”å› â†’ åˆ©ç”¨ç‡æœ€å¤§åŒ–ã€‚

---

## â­ 3. å¤§æ•°ç»„/å¤§çŸ©é˜µ â†’ é¿å… pickle â†’ ç”¨å…±äº«å†…å­˜

ä¾‹å¦‚ï¼š

```python
from multiprocessing import shared_memory
```

é€‚åˆ NumPyã€‚

---

## â­ 4. æ¯ä¸ª worker å†…éƒ¨å¤ç”¨çŠ¶æ€ï¼ˆå¦‚æ¨¡å‹ï¼‰

é€šè¿‡ `initializer` æˆ– `splitted data`ã€‚

---

## â­ 5. é¿å…åœ¨ worker å†…æ‰“å°å¤§é‡è¾“å‡º

æ‰“å°ä¼šé€ æˆ IPC æ‰©å¤§ç“¶é¢ˆï¼Œå¤§è§„æ¨¡ Pool ä¼šå˜æ…¢ã€‚

---

# ğŸ“¦ äº”ã€å¸¸è§é™·é˜±ï¼ˆè¸©å‘æ€»ç»“ï¼‰

|é—®é¢˜|åŸå› |è§£å†³æ–¹æ¡ˆ|
|---|---|---|
|â€œRuntimeError: freeze_support()â€|Windows spawn æ¨¡å¼|åŠ  main guard|
|å¡ä½ä¸é€€å‡º|æœªè°ƒç”¨ `close()/join()`|å¿…é¡» `pool.close()`|
|Worker å´©æºƒä½†æ²¡æœ‰å¼‚å¸¸|å¼‚å¸¸åœ¨å­è¿›ç¨‹|æ•è· trace / logging|
|è¶…æ…¢|pickle å¤§å¯¹è±¡|shared memory / initializer|
|æ­»é”|ä½¿ç”¨ Queue/File Lock|åªä¼ å¯åºåˆ—åŒ–å¯¹è±¡|

---

# ğŸ§® å…­ã€æ€§èƒ½è°ƒä¼˜æ¨¡æ¿ï¼ˆæœ€ä¼˜æ¨¡å¼ï¼‰

```python
from multiprocessing import Pool, cpu_count

def task(x):
    return x * x

def init():
    # load model / open db
    pass

if __name__ == "__main__":
    data = list(range(1000000))
    nproc = cpu_count()

    with Pool(nproc, initializer=init) as pool:
        result = pool.map(task, data, chunksize=1000)
```

è¿™æ˜¯å¤§é‡ä»»åŠ¡çš„æœ€é«˜æ•ˆå†™æ³•ã€‚

---

# ğŸ¯ ä¸ƒã€æ˜¯å¦åº”è¯¥æ¢æˆ `ProcessPoolExecutor`ï¼Ÿ

ä» Python 3.10 å¼€å§‹å®˜æ–¹æ¨èï¼š

- æ–°é¡¹ç›®å°½é‡ä½¿ç”¨ `concurrent.futures.ProcessPoolExecutor`
    
- è¯­æ³•æ›´ç®€æ´ï¼ŒåŠŸèƒ½ç­‰ä»·
    
- æ›´å¤š feature
    

ä¾‹å¦‚ï¼š

```python
from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor() as ex:
    results = list(ex.map(task, data))
```

---

# Example 1

ä¸‹é¢ç»™ä½ ä¸€ä¸ª **ç”Ÿäº§çº§ã€æ€§èƒ½ä¼˜åŒ–å®Œæ•´ã€å¯ç›´æ¥å¥—ç”¨çš„ `multiprocessing.Pool.starmap` æœ€ä½³æ¨¡æ¿**ã€‚  
åŒ…å«ï¼šåˆå§‹åŒ–ã€chunksize ä¼˜åŒ–ã€é”™è¯¯å¤„ç†ã€worker å¤ç”¨ã€Windows/macOS å…¼å®¹ã€‚

---

# ğŸš€ `Pool.starmap` æœ€ä½³æ¨¡æ¿ï¼ˆå¯ç›´æ¥ç”¨ï¼‰

```python
import os
from multiprocessing import Pool, cpu_count

# ==========================
# 1. å…¨å±€èµ„æºåˆå§‹åŒ–ï¼ˆå¯é€‰ï¼‰
# ==========================
def init_worker():
    """
    æ¯ä¸ª worker è¿›ç¨‹å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ã€‚
    é€‚åˆåŠ è½½æ¨¡å‹ã€å¤§æ–‡ä»¶ã€æ•°æ®åº“è¿æ¥ç­‰ã€‚
    """
    global config
    config = {"pid": os.getpid()}   # ç¤ºä¾‹ï¼šæ¯ä¸ª worker æœ‰ä¸“å±çŠ¶æ€
    # ä¾‹å¦‚ï¼šglobal model; model = load_model()
    # ä¾‹å¦‚ï¼šglobal db; db = connect_db()
    # ä¾‹å¦‚ï¼šåŠ è½½å¤§æ•°ç»„ã€åªè¯»æ•°æ®


# ==========================
# 2. Worker ä»»åŠ¡å‡½æ•°
# ==========================
def worker_task(a, b, c):
    """
    starmap éœ€è¦æ¥å—å¤šä¸ªå‚æ•°ã€‚
    æ‰€æœ‰ä»»åŠ¡å¿…é¡»å¯ pickleã€‚
    """
    pid = config["pid"]   # ä½¿ç”¨ worker çš„åˆå§‹åŒ–èµ„æº
    result = (a + b) * c
    return pid, result


# ==========================
# 3. ä¸»æµç¨‹
# ==========================
def main():
    # æ„é€ å‚æ•°ï¼šæ¯é¡¹æ˜¯ä¸€ä¸ª tuple
    tasks = [(i, i*2, i+1) for i in range(100000)]

    # è‡ªåŠ¨æœ€ä½³ chunksizeï¼ˆç»éªŒå…¬å¼ï¼‰
    nproc = cpu_count()
    chunksize = max(1, len(tasks) // (nproc * 4))

    print(f"Using {nproc} processes with chunksize={chunksize}")

    with Pool(
        processes=nproc,
        initializer=init_worker   # é«˜æ€§èƒ½å…³é”®ï¼šæ¯ worker é¢„è½½èµ„æº
    ) as pool:

        # starmap å¥åº·çš„é«˜æ€§èƒ½ç”¨æ³•
        results = pool.starmap(worker_task, tasks, chunksize=chunksize)

    # è¾“å‡ºå‰ 10 ä¸ªç»“æœ
    for r in results[:10]:
        print(r)


if __name__ == "__main__":
    main()
```

---

# ğŸ§  ä¸ºä»€ä¹ˆè¿™æ˜¯æœ€ä½³æ¨¡æ¿ï¼Ÿ

## âœ” 1. `initializer` æ˜¾è‘—æå‡æ€§èƒ½

è€Œä¸æ˜¯æŠŠå¤§å¯¹è±¡ pickle å¤šæ¬¡ä¼ ç»™ workerã€‚

é€‚ç”¨äºï¼š

- æ¨¡å‹åŠ è½½ï¼ˆPyTorch/TensorFlowï¼‰
    
- å¤§å‹ JSON / é…ç½®
    
- æ•°æ®åº“è¿æ¥
    
- å¤§å‹ NumPy æ•°ç»„ï¼ˆåªè¯»ï¼‰
    
- é¢„å¤„ç†è§„åˆ™ã€tokenizer ç­‰
    

---

## âœ” 2. è‡ªåŠ¨è®¡ç®—æœ€ä½³ `chunksize`

é»˜è®¤ `chunksize=1` éå¸¸æ…¢ï¼Œä¼šé€ æˆï¼š

- IPC é€šä¿¡æˆæœ¬é«˜
    
- pickle é¢‘ç‡å¤ªé«˜
    

é€šè¿‡ï¼š

```
chunksize = len(tasks) // (nproc * 4)
```

å¯æé€Ÿ 2ï½10 å€ã€‚

---

## âœ” 3. ä½¿ç”¨ `starmap` è€Œä¸æ˜¯è‡ªå·±å°è£…å‚æ•°

æ›´ç®€æ´ã€æ€§èƒ½æ›´å¥½ï¼š

```python
pool.starmap(func, [(arg1, arg2, arg3), ...])
```

---

## âœ” 4. ä½¿ç”¨ `with Pool(...)` è‡ªåŠ¨ close/join

é¿å…é—ç•™åƒµå°¸è¿›ç¨‹ã€‚

---

## âœ” 5. å…¼å®¹ Windows/macOSï¼ˆspawnï¼‰

æ¨¡æ¿ä¸­ main guard å¿…é¡»æœ‰ï¼š

```python
if __name__ == "__main__":
```

---

# âš¡ ä¸“ä¸šç‰ˆï¼šåŠ å…¥é”™è¯¯å¤„ç†ï¼ˆæ›´å®‰å…¨ï¼‰

```python
def worker_task(a, b, c):
    try:
        return (a + b) * c
    except Exception as e:
        return {"error": str(e), "args": (a, b, c)}
```

æ¥ç€è¿‡æ»¤ç»“æœã€‚

---

# Example 2

ä¸‹é¢ç»™ä½ ä¸€ä¸ª **å®Œå…¨å¯ç”¨ã€ç¨³å®šã€é«˜æ€§èƒ½ã€æ”¯æŒ tqdm è¿›åº¦æ¡çš„ `Pool.starmap` æœ€ä½³æ¨¡æ¿**ã€‚

`multiprocessing.Pool.starmap()` æœ¬èº«**ä¸èƒ½**ç›´æ¥è·å–ä»»åŠ¡è¿›åº¦ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ç”¨ `imap_unordered()` æ¥æ¨¡æ‹Ÿ `starmap` è¡Œä¸ºï¼Œå¹¶å®ç°å®æ—¶è¿›åº¦æ›´æ–°ã€‚

æ¨¡æ¿åŒ…å«ï¼š

- tqdm å®æ—¶è¿›åº¦æ¡
    
- starmap å¤šå‚æ•°è°ƒç”¨
    
- initializer èµ„æºåŠ è½½
    
- æœ€ä½³ chunksize é…ç½®
    
- Windows/macOS å…¼å®¹
    

---

# ğŸš€ **å¸¦ tqdm è¿›åº¦æ¡çš„ starmapï¼ˆä¸“ä¸šç‰ˆæœ¬ï¼‰**

```python
import os
from multiprocessing import Pool, cpu_count
from tqdm import tqdm

# ==========================
# 1. æ¯ä¸ª worker çš„åˆå§‹åŒ–ï¼ˆå¯é€‰ï¼‰
# ==========================
def init_worker():
    global state
    state = {
        "pid": os.getpid()
    }
    # å¦‚éœ€åŠ è½½æ¨¡å‹/æ•°æ®åº“/å¤§å‹æ–‡ä»¶ï¼Œå¯åœ¨æ­¤è¿›è¡Œ
    # global model; model = load_model()


# ==========================
# 2. Worker æ‰§è¡Œå‡½æ•°
# ==========================
def worker_task(args):
    """
    ç”±äºæˆ‘ä»¬è¦ç”¨ imap è€Œé starmapï¼Œ
    æ‰€ä»¥æŠŠå‚æ•°æ‰“åŒ…æˆ tupleï¼Œworker_task(args) è§£åŒ…å³å¯ã€‚
    """
    a, b, c = args
    pid = config["pid"]
    return pid, (a + b) * c


# ==========================
# 3. æ¨¡æ‹Ÿ starmap + tqdm è¿›åº¦æ¡
# ==========================
def starmap_with_tqdm(pool, func, iterable, chunksize):
    """
    ä½¿ç”¨ imap_unordered æ¨¡æ‹Ÿ starmapï¼Œå¹¶æ·»åŠ  tqdm è¿›åº¦æ¡ã€‚
    """
    total = len(iterable)
    results = []

    for r in tqdm(
        pool.imap_unordered(lambda args: func(*args), iterable, chunksize=chunksize),
        total=total,
        desc="Processing",
    ):
        results.append(r)

    return results


# ==========================
# 4. ä¸»æµç¨‹
# ==========================
def main():
    tasks = [(i, i * 2, i + 1) for i in range(50000)]

    nproc = cpu_count()
    chunksize = max(1, len(tasks) // (nproc * 4))

    print(f"Using {nproc} processes with chunksize={chunksize}")

    with Pool(
        processes=nproc,
        initializer=init_worker,
    ) as pool:

        results = starmap_with_tqdm(pool, worker_task, tasks, chunksize)

    print("å‰ 10 ä¸ªç»“æœ:")
    for x in results[:10]:
        print(x)


if __name__ == "__main__":
    main()
```

---

# ğŸ§  ä¸ºä»€ä¹ˆè¦ç”¨ `imap_unordered` è€Œä¸æ˜¯ starmapï¼Ÿ

å› ä¸ºï¼š

- `starmap()` â†’ **ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰ç»“æœ** â†’ æ— æ³•æ›´æ–°è¿›åº¦æ¡
    
- `imap()` / `imap_unordered()` â†’ **åƒè¿­ä»£å™¨é€ä¸ªè¿”å›** â†’ å¯å®æ—¶æ›´æ–°è¿›åº¦
    

`imap_unordered()` çš„ä¼˜ç‚¹ï¼š

- å…ˆå®Œæˆçš„ä»»åŠ¡å…ˆè¿”å›ï¼ˆæ›´å¿«ï¼‰
    
- ä¸ tqdm å®Œç¾ç»“åˆ
    
- æ€§èƒ½æ¯” `starmap` æ›´å¥½ï¼ˆæ— éœ€å…ˆ pack å‚æ•°ï¼‰
    

---

# âš¡ æ€§èƒ½ç‰¹æ€§

- é€‚åˆå¤§é‡å°ä»»åŠ¡
    
- é¿å…é¢‘ç¹ pickleï¼šä½¿ç”¨åˆç† chunk size
    
- è‡ªåŠ¨é€‚é… CPU æ ¸æ•°
    
- initializer è®© worker ç‹¬ç«‹åŠ è½½èµ„æºï¼Œå¤§å¹…æå‡æ€§èƒ½
    

---

# ğŸ“˜ å¦‚æœä½ éœ€è¦æ›´å¼ºç‰ˆæœ¬

æˆ‘å¯ä»¥ç»§ç»­ç»™ä½ ï¼š

âœ” å¸¦ **é¡ºåºä¸€è‡´æ€§** çš„ tqdm starmapï¼ˆä¿è¯ starmap æ¬¡åºï¼‰  
âœ” å¸¦ **å¼‚å¸¸æ•è·** çš„ tqdm starmap  
âœ” **æ”¯æŒ NumPy å…±äº«å†…å­˜** çš„å¹¶è¡Œæ¨¡æ¿  
âœ” **CPU å¯†é›†ä»»åŠ¡æœ€ä½³å‚æ•°è‡ªåŠ¨è®¡ç®—**  
âœ” **IO å¯†é›†ä»»åŠ¡æœ€ä½³çº¿ç¨‹+è¿›ç¨‹ç»„åˆæ± **

å‘Šè¯‰æˆ‘ä½ çš„ä»»åŠ¡ç±»å‹ï¼ˆCPUã€IOã€æœºå™¨å­¦ä¹ ã€æ•°æ®åº“ç­‰ï¼‰ï¼Œæˆ‘ç»™ä½ æœ€ä¼˜æ¨¡æ¿ã€‚