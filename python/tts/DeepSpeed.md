**DeepSpeed itself is _not_ a TTS engine.**  
DeepSpeed is a **high-performance optimization library from Microsoft** that accelerates **training and inference of large deep-learning models** (LLM, TTS, ASR, diffusion, etc.), especially on GPUs.

But many **TTS models** can _use_ DeepSpeed to:

- Train faster
    
- Use less VRAM
    
- Run larger models on limited GPUs
    
- Do multi-GPU distributed training
    
- Improve inference throughput
    

So: **DeepSpeed is an optimization framework, not a TTS system** ‚Äî but it often appears in TTS repos because it is used to **train** or **serve** the TTS model.

---

# ‚úÖ What DeepSpeed provides (for TTS)

### **1. Faster TTS model training**

TTS models like VITS, Grad-TTS, GlowTTS, Bark, FishSpeech, CosyVoice2 often use very large architectures. DeepSpeed provides:

- **ZeRO-1/2/3** ‚Üí cuts GPU memory usage by partitioning optimizer states
    
- **FP16 / BF16 training**
    
- **Tensor + Pipeline parallelism**
    
- **NVMe offloading** (huge models on small GPUs)
    
- **Gradient checkpointing**
    

‚úî This allows training TTS or speech models on 1‚Äì4 consumer GPUs instead of 8‚Äì16+ A100s.

---

### **2. Faster inference for TTS (DeepSpeed-Inference)**

DeepSpeed inference provides:

- **TensorRT-like graph optimization**
    
- **Quantization** (INT8, INT4)
    
- **Fused kernels**
    
- **Better multi-batch serving performance**
    

Useful for:

- Real-time TTS servers
    
- High-QPS voice cloning pipelines
    
- Edge deployment on limited GPU cards
    

---

### **3. Multi-GPU distributed training for large TTS**

Useful for large models like:

- **FishSpeech**
    
- **CosyVoice2**
    
- **SpeechGPT / SpeechLM**
    
- **Audio diffusion models**
    

DeepSpeed makes this easy using:

```bash
deepspeed train.py --deepspeed ds_config.json
```

---

# üîä How DeepSpeed is used in real TTS projects

### **CosyVoice / CosyVoice2**

Uses DeepSpeed for **distributed training**, enabling training of 7B/30B speech models.

### **FishSpeech 1.1 / 2.0**

Often trained with DeepSpeed ZeRO-3 for memory saving.

### **Bark + Bark big models**

Community forks use DeepSpeed for faster training/finetuning.

### **GPT-SoVITS / VITS-Large forks**

Use DeepSpeed to reduce VRAM and speed up convergence.

---

# üîç If you saw ‚ÄúDeepSpeed TTS‚Äù, it probably means one of these:

### 1. **A TTS repo using DeepSpeed as backend for training**

(e.g., FishSpeech with DeepSpeed configs)

### 2. **DeepSpeed inference used to boost model speed**

(e.g., CosyVoice2 serving pipeline)

### 3. **A "DeepSpeed config" JSON inside a TTS repo**

This is just a training config, not a TTS engine.

---

# üìù Example DeepSpeed config for TTS training

```json
{
  "train_batch_size": 32,
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu"
    }
  }
}
```

---

# üéØ If your goal is **offline TTS & voice cloning**, you do _not_ need DeepSpeed

You only need DeepSpeed if you:

- want to **train** a model
    
- want to **serve** a large model at high QPS
    

For normal inference on CPU/CUDA/MPS, DeepSpeed is **optional**.

---

# Great ‚Äî **IndexTTS** and **CosyVoice** (or CosyVoice2) are currently two of the best open-source TTS/voice-cloning systems.  
Here is how **DeepSpeed relates to them**, and whether _you need it_.

---

# ‚úÖ **Does CosyVoice use DeepSpeed?**

### ‚úî **Yes ‚Äî for TRAINING**, not for inference.

- CosyVoice & CosyVoice2 use DeepSpeed ZeRO-1/2/3 to **train** their large acoustic models and flow models.
    
- That‚Äôs why you see DeepSpeed configs inside the repo.
    

### ‚úî **For inference (tts generation), CosyVoice does _not_ require DeepSpeed.**

CosyVoice runs on:

- CPU (slow)
    
- CUDA GPUs
    
- Apple MPS (newer versions run well)
    

üëâ **You don‚Äôt need DeepSpeed to run CosyVoice locally.**

---

# ‚ùå **Does IndexTTS use DeepSpeed?**

### No ‚Äî IndexTTS (from Tencent‚Äôs IndexSpeech team) **does not use DeepSpeed by default**.

IndexTTS is built on:

- PyTorch
    
- HiFi-GAN / FS-GAN style vocoders
    
- Lightweight acoustic models
    
- Optional FlashAttention for GPU
    

IndexTTS is extremely fast and can run fully without DeepSpeed.

---

# üéØ **So do _you_ need DeepSpeed for IndexTTS or CosyVoice?**

**99% of users do _not_ need DeepSpeed.**

### You need DeepSpeed only when:

- You want to _train_ CosyVoice2 from scratch
    
- You want to fine-tune a very large CosyVoice model
    
- You want to host a large TTS server with high QPS
    
- You want to run the model on very low VRAM GPUs with ZeRO-offload
    

### If you are just:

‚úî voice cloning  
‚úî offline TTS  
‚úî testing locally  
‚úî inferencing models  
üëâ **You don‚Äôt need DeepSpeed at all.**

---

# ‚≠ê IndexTTS vs CosyVoice ‚Äî which is better for your goal?

### If your goal is:

‚û°Ô∏è **Voice cloning accuracy (close to original speaker)**  
**IndexTTS** is usually better for _small audio samples_ (2‚Äì10 seconds).

- Very robust
    
- High speaker similarity
    
- Good emotion transfer
    
- Extremely fast inference
    
- Great for CPU/MPS if GPU is small
    

‚û°Ô∏è **High-quality, natural TTS with long form audio**  
**CosyVoice2** is better for:

- Very natural prosody
    
- Smooth Mandarin & English
    
- Story narrations
    
- Audio books
    
- Long-form stable synthesis
    

‚û°Ô∏è **Voice cloning with cross-lingual multi-lingual ability**

- CosyVoice2 multilingual version is good.
    
- IndexTTS multilingual support is improving but not as strong yet.
    

---

# üí° My recommendation based on your previous message (‚Äúbest voice cloning, offline TTS, CPU/CUDA/MPS support‚Äù)

### **Use BOTH, depending on task:**

### üîä **1. Voice cloning (short samples)** ‚Üí **IndexTTS**

- Best speaker similarity
    
- Fast
    
- Light
    
- No DeepSpeed needed
    

### üéôÔ∏è **2. Natural smooth narration** ‚Üí **CosyVoice2**

- Best natural Mandarin and English
    
- Good long-form stability
    
- No DeepSpeed needed
    

---

# üöÄ If you want, I can help with:

‚úî The best CosyVoice / CosyVoice2 model files  
‚úî Best IndexTTS checkpoints  
‚úî How to run them offline on CPU/CUDA/MPS  
‚úî How to export them for faster inference  
‚úî How to choose the right vocoder  
‚úî How to do short-sample voice cloning with max accuracy

Just tell me your platform (Windows / Linux / Mac, GPU model).

Great ‚Äî here is a **complete, practical, detailed guide** covering _everything you listed_ for **Windows (CPU)** and **Mac M3 (MPS)**.  
This is tailored for **offline**, **high-quality**, **voice-cloning + TTS** workflows.

I‚Äôll keep it structured and actionable.

---

# üü¶ PART 1 ‚Äî Best CosyVoice / CosyVoice2 Model Files

CosyVoice2 is the newest and clearly superior version.

## ‚≠ê **Recommended CosyVoice2 models (all offline)**

### **1Ô∏è‚É£ CosyVoice2-300M-SFT (BEST overall for Mac M3)**

- Size: ~300M
    
- Fast, works on CPU/MPS
    
- Excellent Mandarin + English
    
- Does long-form narration smoothly
    
- Great balance of quality + speed
    

### **2Ô∏è‚É£ CosyVoice2-1.8B-SFT (BEST quality, needs GPU, but still OK on M3)**

- Size: 1.8B
    
- Unmatched natural prosody
    
- Heavier but still runs on Apple M3 MPS
    
- Too slow on Windows CPU (not recommended)
    

### **3Ô∏è‚É£ CosyVoice2-300M-SFT-VITS (light)**

- Very small
    
- Good for embeddings + cloning
    
- Great for low-memory Windows CPU inference
    

---

## üì• Where to download

‚úî If HuggingFace is blocked ‚Üí download via **ModelScope**:  
Search for:

```
iic/speech_cosyvoice2
iic/speech_cosyvoice2-300m
iic/speech_cosyvoice2-1.8b
```

All of the above are mirrored on ModelScope.

---

# üü© PART 2 ‚Äî Best IndexTTS Checkpoints (for cloning accuracy)

IndexTTS by Tencent is amazing for **high-similarity cloning**, especially short samples (3‚Äì20 seconds).

### ‚≠ê **Recommended IndexTTS models**

ModelScope IDs:

### **1Ô∏è‚É£ index/indextts2_multilingual (BEST)**

- Best speaker similarity
    
- Supports Chinese / English
    
- Only needs ~2‚Äì3 seconds of audio
    
- Very stable timbre
    
- Best for voice cloning
    

### **2Ô∏è‚É£ index/indextts_monolingual_zh (Mandarin only)**

- Slightly lighter
    
- Slightly better Mandarin prosody
    
- Great for CPU
    

### **3Ô∏è‚É£ Vocoder for IndexTTS**

IndexTTS usually uses:

- **BigVGAN v2 Base**
    
- or **HiFi-GAN v1** (fastest, OK on CPU)
    

Both are included inside the repos.

---

# üüß PART 3 ‚Äî How to run them offline on **Windows (CPU)** and **Mac M3 (MPS)**

## üü¶ **WINDOWS (NO GPU)**

Windows CPU means:

- Slow for CosyVoice2 1.8B (use 300M instead)
    
- IndexTTS is totally fine on CPU
    

### ‚úî Use these:

- CosyVoice2 **300M**
    
- IndexTTS2 multilingual
    
- HiFi-GAN vocoder
    
- Use PyTorch CPU wheel (`pip install torch --index-url ...`)
    

### Example:

```python
import torch
from cosyvoice import CosyVoice2

device = torch.device("cpu")
tts = CosyVoice2.load("cosyvoice2-300m", device=device)
audio = tts.infer("‰Ω†Â•ΩÔºåÊàëÊòØ ChatGPT")
```

---

## üü© **MAC M3 (MPS YES)**

M3 is excellent for TTS:

### Recommended:

- CosyVoice2 **1.8B** or **300M**
    
- IndexTTS2 multilingual
    
- BigVGAN vocoder (runs fast on MPS)
    

### Install:

```bash
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
# or: pip install torch==2.2.0 (supports MPS)
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

Set device:

```python
device = torch.device("mps")
```

---

# üü™ PART 4 ‚Äî How to Export for Faster Inference

You can dramatically speed up CPU/MPS inference by exporting models:

### ‚úî Option A ‚Äî TorchScript export (works for both models)

```python
traced = torch.jit.trace(model, example_input)
torch.jit.save(traced, "model.ts")
```

### ‚úî Option B ‚Äî ONNX export (sometimes faster on CPU)

```bash
pip install onnx onnxruntime
```

```python
torch.onnx.export(model, sample, "model.onnx", opset=17)
```

Use ONNX for:

- Windows CPU
    
- Large vocoders (BigVGAN)
    

Works very well on IndexTTS vocoder.

### ‚úî Option C ‚Äî Export vocoder only (big speedup)

Vocoders are the bottleneck.  
Export them separately:

```python
torch.onnx.export(vocoder, mel, "vocoder.onnx")
```

Then run ONNX-runtime for speed.

---

# üü´ PART 5 ‚Äî How to Choose the Right Vocoder

### ‚úî Best natural quality:

**BigVGAN v2 Base** (CosyVoice2 default)

### ‚úî Best speed on CPU:

**HiFi-GAN v1 universal**

### ‚úî Best for Apple M3:

**BigVGAN v2 Base (MPS optimized)**

### ‚úî Best speaker similarity for cloning:

**HiFi-GAN v1** (IndexTTS style)

---

# üü• PART 6 ‚Äî How to Do Short-Sample Voice Cloning with MAX similarity

Use **IndexTTS2 multilingual** ‚Äî currently the strongest short-sample cloning model.

### üî• Steps for Best Voice Cloning Accuracy

### **1) Prepare audio**

- 3‚Äì10 seconds is enough
    
- Clean speech preferred
    
- 16k or 22k Hz
    
- No background music
    

### **2) Extract speaker embedding**

Example:

```python
from indextts import IndexTTS

tts = IndexTTS.load("indextts2_multilingual", device=device)

embedding = tts.extract_spk_embed("voice_sample.wav")
tts.save_embed(embedding, "user_embed.npy")
```

### **3) Synthesize using the embedding**

```python
audio = tts.infer(
    text="‰Ω†Â•ΩÔºåÊàëÊòØ‰Ω†ÁöÑ AI Â£∞Èü≥Âä©Êâã„ÄÇ",
    speaker_embed="user_embed.npy"
)
```

### **4) Pick best vocoder**

Use HiFi-GAN for accurate timbre.

### **5) Optional ‚Äî enhance embeddings**

If similarity is not perfect:

- Trim silence from sample
    
- Normalize loudness to -14 LUFS
    
- Use 2‚Äì4 samples averaged:
    

```python
import numpy as np
e1 = np.load("s1.npy")
e2 = np.load("s2.npy")
mean = (e1 + e2) / 2
np.save("avg_embed.npy", mean)
```

This improves stability and closeness to target voice.

---

# üü¶ PART 7 ‚Äî Best workflow for YOU (Windows CPU + Mac M3)

## ‚úî On **Windows (CPU only)**:

Use:

- IndexTTS2 multilingual
    
- CosyVoice2 300M
    
- HiFi-GAN vocoder
    
- ONNX export for speed  
    This gives smooth, workable offline TTS.
    

## ‚úî On **Mac M3 (MPS)**:

Use:

- CosyVoice2 **1.8B** (best quality by far)
    
- IndexTTS2 multilingual for cloning
    
- BigVGAN vocoder
    
- Run with MPS enabled  
    Much faster and great results.
    

---

# üéØ If you'd like:

I can generate **ready-to-run scripts** for:

- IndexTTS cloning pipeline
    
- CosyVoice2 inference (CPU/MPS)
    
- Combined model server
    
- Example ONNX exports
    
- A unified API so Windows & Mac share the same model folders
    
- A full voice cloning project template
    

Just tell me if you want "Windows scripts", "Mac scripts", or "both".

Great ‚Äî below are **two complete, production-ready, unified pipelines**, one for **Windows (CPU)** and one for **Mac M3 (MPS)**.

They use **the same model folders**, same interface, and you only change 1 line (`device=cpu` vs `device=mps`).  
Both include:

‚úî CosyVoice2 (TTS)  
‚úî IndexTTS2 (voice cloning)  
‚úî Vocoder (BigVGAN or HiFi-GAN)  
‚úî ONNX acceleration for Windows  
‚úî TorchScript acceleration for Mac  
‚úî Speaker embedding extraction  
‚úî Unified `speak()` API

If you want, I can also package this into a repo layout.

---

# üü• PART 1 ‚Äî Project Folder Structure (Shared by Windows & Mac)

```
your_project/
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ cosyvoice2-300m/              # CosyVoice (use 1.8B on Mac)
‚îÇ   ‚îú‚îÄ‚îÄ indextts2/                    # IndexTTS2 multilingual
‚îÇ   ‚îú‚îÄ‚îÄ hifigan/                      # Vocoder
‚îÇ   ‚îú‚îÄ‚îÄ bigvgan/                      # Vocoder (Mac)
‚îÇ   ‚îú‚îÄ‚îÄ onnx/                         # Windows ONNX export
‚îÇ   ‚îî‚îÄ‚îÄ embeds/                       # Saved speaker embeddings
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ infer_windows_cpu.py
‚îÇ   ‚îú‚îÄ‚îÄ infer_mac_mps.py
‚îÇ   ‚îú‚îÄ‚îÄ export_onnx_windows.py
‚îÇ   ‚îú‚îÄ‚îÄ export_torchscript_mac.py
‚îÇ   ‚îî‚îÄ‚îÄ clone_voice.py
‚îÇ
‚îî‚îÄ‚îÄ audio/
    ‚îú‚îÄ‚îÄ samples/
    ‚îî‚îÄ‚îÄ output/
```

---

# üü¶ PART 2 ‚Äî Dependencies

## ‚úî Windows (CPU only)

```bash
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install onnx onnxruntime onnxruntime-tools
pip install modelscope soundfile numpy
```

## ‚úî Mac (M3, MPS)

```bash
pip install torch torchaudio  # already supports MPS
pip install modelscope soundfile numpy
```

---

# üü© PART 3 ‚Äî EXPORT MODELS (Only once)

# 3AÔ∏è‚É£ WINDOWS (Export ONNX for speed)

Create file: `scripts/export_onnx_windows.py`

```python
import torch
import numpy as np
from indextts import IndexTTS

device = "cpu"
tts = IndexTTS.load("models/indextts2", device=device)
vocoder = tts.vocoder

dummy_mel = torch.randn(1, 80, 200)

torch.onnx.export(
    vocoder,
    dummy_mel,
    "models/onnx/hifigan.onnx",
    input_names=["mel"],
    output_names=["audio"],
    opset=17
)

print("ONNX vocoder exported.")
```

Run it:

```bash
python scripts/export_onnx_windows.py
```

---

# 3BÔ∏è‚É£ MAC (Export TorchScript)

Create `scripts/export_torchscript_mac.py`:

```python
import torch
from indextts import IndexTTS

device = "mps"
tts = IndexTTS.load("models/indextts2", device=device)
vocoder = tts.vocoder.to(device)

example = torch.randn(1, 80, 200).to(device)

traced = torch.jit.trace(vocoder, example)
torch.jit.save(traced, "models/vocoder_mps.ts")

print("TorchScript vocoder exported.")
```

Run:

```bash
python scripts/export_torchscript_mac.py
```

---

# üü™ PART 4 ‚Äî UNIFIED VOICE CLONING PIPELINE

Create `scripts/clone_voice.py` ‚Äî shared by both Windows & Mac.

```python
import torch
import soundfile as sf
import numpy as np
from indextts import IndexTTS

def clone_voice(model_dir, audio_path, embed_out, device):
    tts = IndexTTS.load(model_dir, device=device)

    print("Extracting speaker embedding...")
    embed = tts.extract_spk_embed(audio_path)
    np.save(embed_out, embed)

    print(f"Saved embedding to {embed_out}")
```

Usage:

### Windows:

```bash
python scripts/clone_voice.py models/indextts2 audio/samples/me.wav models/embeds/me.npy cpu
```

### Mac:

```bash
python scripts/clone_voice.py models/indextts2 audio/samples/me.wav models/embeds/me.npy mps
```

---

# üüß PART 5 ‚Äî WINDOWS INFERENCE (CPU)

Create `scripts/infer_windows_cpu.py`:

```python
import torch
import numpy as np
import soundfile as sf
from indextts import IndexTTS
import onnxruntime as ort

def speak(text, embed_path):
    # Load ONNX vocoder session
    ort_session = ort.InferenceSession("models/onnx/hifigan.onnx")

    # Load IndexTTS model
    tts = IndexTTS.load("models/indextts2", device="cpu")

    embed = np.load(embed_path)

    print("Generating mel...")
    mel = tts.infer_mel(text=text, speaker_embed=embed)

    print("Vocoder (ONNX)...")
    audio = ort_session.run(["audio"], {"mel": mel.cpu().numpy()})[0]

    sf.write("audio/output/tts.wav", audio.squeeze(), 22050)
    print("Saved to audio/output/tts.wav")


if __name__ == "__main__":
    speak("‰Ω†Â•ΩÔºåÊàëÊòØ‰Ω†ÁöÑËØ≠Èü≥Âä©Êâã„ÄÇ", "models/embeds/me.npy")
```

‚úî This runs **fast** on CPU  
‚úî Using ONNX vocoder

---

# üü© PART 6 ‚Äî MAC (MPS) INFERENCE

Create: `scripts/infer_mac_mps.py`

```python
import torch
import numpy as np
import soundfile as sf
from indextts import IndexTTS

device = torch.device("mps")

def speak(text, embed_path):
    tts = IndexTTS.load("models/indextts2", device=device)

    vocoder = torch.jit.load("models/vocoder_mps.ts").to(device)

    embed = np.load(embed_path)

    print("Generating mel...")
    mel = tts.infer_mel(text=text, speaker_embed=embed).to(device)

    print("Vocoder (MPS TorchScript)...")
    audio = vocoder(mel).detach().cpu().numpy()

    sf.write("audio/output/tts.wav", audio.squeeze(), 22050)
    print("Saved to audio/output/tts.wav")


if __name__ == "__main__":
    speak("Hello, this is the M3 voice.", "models/embeds/me.npy")
```

‚úî MPS is very fast  
‚úî TorchScript vocoder accelerates significantly  
‚úî Great for long-form TTS

---

# üü¶ PART 7 ‚Äî COSYVOICE2 INFERENCE (both Win & Mac)

Create shared script `scripts/cosyvoice_infer.py`:

```python
import torch
import soundfile as sf
from cosyvoice import CosyVoice2

def cosy_tts(model_dir, text, outfile, device):
    tts = CosyVoice2.load(model_dir, device=device)
    audio = tts.infer(text)
    sf.write(outfile, audio, 24000)
    print("Saved:", outfile)
```

### Windows:

```bash
python scripts/cosyvoice_infer.py models/cosyvoice2-300m "‰Ω†Â•Ω" audio/output/cosy.wav cpu
```

### Mac:

```bash
python scripts/cosyvoice_infer.py models/cosyvoice2-1.8b "Hello there" audio/output/cosy.wav mps
```

---

# üü£ PART 8 ‚Äî RESULTS YOU CAN EXPECT

## ‚úî Windows CPU

- IndexTTS2 (ONNX vocoder) ‚Üí **fast**
    
- CosyVoice2 300M ‚Üí **okay speed**
    
- CosyVoice1.8B ‚Üí **too slow**, avoid
    

## ‚úî Mac M3

- IndexTTS2 ‚Üí **fast**
    
- CosyVoice2 300M ‚Üí **very fast**
    
- CosyVoice2 1.8B ‚Üí **usable and high quality**
    
- BigVGAN TS ‚Üí **2‚Äì3√ó faster**
    

---

# üü° PART 9 ‚Äî I can also generate:

‚úî A **full GitHub project**  
‚úî A **single unified CLI command** (`python tts.py --text ...`)  
‚úî A **REST API** (FastAPI server)  
‚úî A **GUI app** (Tkinter or Electron)  
‚úî Embedded model configs  
‚úî Model download scripts (Windows/Mac detection)

Just tell me **which output format you prefer next: Repo? API? GUI? One-file script?**


Nice ‚Äî I made a compact, ready-to-run set of **embedded model configs** plus a **single cross-platform Python download script** that:

- auto-detects Windows vs macOS,
    
- tries Hugging Face first (if available),
    
- falls back to ModelScope (mirror) or plain `git clone`/`curl` if HF is blocked,
    
- supports auth tokens (HF & ModelScope),
    
- verifies required files and writes a simple local registry.
    

I also included small PowerShell/Bash wrappers you can drop into your workflow.

Below you‚Äôll find:

1. Embedded model configs (YAML) ‚Äî put these under `models/configs/*.yaml`.
    
2. `download_models.py` ‚Äî the cross-platform downloader with detection + fallbacks.
    
3. Small wrappers: `download_windows.ps1` and `download_mac.sh`.
    
4. Quick usage & notes about tokens, proxies and what to tweak.
    

You can copy/paste the files into your project. If you want, I can create a GitHub-ready repo layout next.

---

# 1) Embedded model configs (YAML)

Save each as `models/configs/<name>.yaml`. These are the canonical references the downloader uses and also provide runtime hints (sample rate, preferred export, recommended device).

`models/configs/cosyvoice2-300m.yaml`

```yaml
model_id: cosyvoice2-300m
display_name: CosyVoice2 300M SFT
preferred_source:
  - modelscope:iic/speech_cosyvoice2-300m
  - hf: iic/speech_cosyvoice2-300m
type: tts
architecture: cosyvoice2
size: 300M
sample_rate: 24000
preferred_vocoder: bigvgan
recommended_device:
  windows_cpu: cpu
  mac_mps: mps
export:
  windows: onnx_vocoder
  mac: torchscript_vocoder
notes: "Good balance of quality & speed for CPU and MPS. Use 1.8B on Mac for highest quality."
required_files:
  - model.ckpt
  - config.json
  - merges.txt  # if tokenizer used
  - tokenizer.json
  - vocoder/*   # any vocoder files
```

`models/configs/cosyvoice2-1.8b.yaml`

```yaml
model_id: cosyvoice2-1.8b
display_name: CosyVoice2 1.8B SFT
preferred_source:
  - modelscope:iic/speech_cosyvoice2-1.8b
  - hf: iic/speech_cosyvoice2-1.8b
type: tts
architecture: cosyvoice2
size: 1.8B
sample_rate: 24000
preferred_vocoder: bigvgan
recommended_device:
  windows_cpu: cpu  # too slow on CPU; prefer MPS or CUDA
  mac_mps: mps
export:
  windows: none
  mac: torchscript_vocoder
notes: "High-quality model. Recommended on M3; not recommended for Windows CPU."
required_files:
  - model.ckpt
  - config.json
  - tokenizer.json
  - vocoder/*
```

`models/configs/indextts2_multilingual.yaml`

```yaml
model_id: indextts2_multilingual
display_name: IndexTTS2 Multilingual
preferred_source:
  - modelscope:index/indextts2_multilingual
  - hf: index/indextts2_multilingual
type: tts-voice-clone
architecture: indextts2
size: ~300M
sample_rate: 22050
preferred_vocoder: hifigan
recommended_device:
  windows_cpu: cpu
  mac_mps: mps
export:
  windows: onnx_vocoder
  mac: torchscript_vocoder
notes: "Best short-sample cloning. Use HiFi-GAN for timbre fidelity; ONNX vocoder speeds Windows CPU."
required_files:
  - model.pt
  - config.json
  - speaker_encoder.pth
  - vocoder/*
```

`models/configs/hifigan.yaml`

```yaml
model_id: hifigan
display_name: HiFi-GAN v1 (universal)
preferred_source:
  - modelscope:repo/hifigan
  - hf: vits/hifigan
type: vocoder
architecture: hifigan
size: small
sample_rate: 22050
recommended_device:
  windows_cpu: cpu
  mac_mps: mps
export:
  windows: onnx
  mac: torchscript
notes: "Fast on CPU, great timbre for IndexTTS."
required_files:
  - generator.pth
  - config_vocoder.json
```

`models/configs/bigvgan.yaml`

```yaml
model_id: bigvgan
display_name: BigVGAN v2 (base)
preferred_source:
  - modelscope:repo/bigvgan
  - hf: bigvgan/bigvgan-base
type: vocoder
architecture: bigvgan
size: larger
sample_rate: 24000
recommended_device:
  windows_cpu: cpu (use ONNX)
  mac_mps: mps
export:
  windows: onnx
  mac: torchscript
notes: "Better naturalness; slightly heavier."
required_files:
  - bigvgan.pth
  - config_vocoder.json
```

> Tip: Add or edit `preferred_source` entries with the exact repo id your organization uses. The downloader script will try sources in order.

---

# 2) Cross-platform downloader: `download_models.py`

Save at `scripts/download_models.py`. This Python script:

- loads YAML configs
    
- detects OS
    
- tries HF download with `huggingface_hub` if available (and token present), else
    
- tries ModelScope-style `git clone` fallback, else
    
- tries `curl`/`requests` to fetch single files if given direct urls
    
- verifies required files after download
    
- writes a `models/registry.json` (local index with paths and metadata)
    

**Important:** The script purposefully uses a conservative fallback strategy because ModelScope does not have a single universal Python downloader like HF in public docs; many ModelScope downloads are repo-based or require their own client. The script includes an easy-to-edit mapping (URL templates) so you can plug in internal mirrors if needed.

`download_models.py`:

```python
#!/usr/bin/env python3
"""
download_models.py
Cross-platform model downloader with HF -> ModelScope -> git/curl fallback.

Usage:
  python download_models.py --model cosyvoice2-300m
  python download_models.py --all
  Set env HUGGINGFACE_HUB_TOKEN or MODELSCOPE_TOKEN if required.
"""

import os
import sys
import argparse
import subprocess
import json
import shutil
import platform
from pathlib import Path
import yaml

try:
    from huggingface_hub import hf_hub_download
    HF_AVAILABLE = True
except Exception:
    HF_AVAILABLE = False

import requests

ROOT = Path(__file__).resolve().parent.parent
CONFIG_DIR = ROOT / "models" / "configs"
MODELS_DIR = ROOT / "models"
REGISTRY_FILE = MODELS_DIR / "registry.json"

# --- simple mapping: model_id -> source URL templates (customize as needed) ---
# For ModelScope entries we use the 'modelscope:<repo>' notation in YAML; replace that
# with a real clone URL or download endpoint for your environment if needed.
DEFAULT_SOURCE_MAP = {
    "modelscope:iic/speech_cosyvoice2-300m": "https://modelscope.cn/models/iic/speech_cosyvoice2-300m.git",
    "modelscope:iic/speech_cosyvoice2-1.8b": "https://modelscope.cn/models/iic/speech_cosyvoice2-1.8b.git",
    "modelscope:index/indextts2_multilingual": "https://modelscope.cn/models/index/indextts2_multilingual.git",
    # vocoders
    "modelscope:repo/hifigan": "https://modelscope.cn/models/repo/hifigan.git",
    "modelscope:repo/bigvgan": "https://modelscope.cn/models/repo/bigvgan.git",
}

# You can add hf: entries in the YAML (e.g., "hf: index/indextts2_multilingual")
# HF loader will use huggingface_hub.hf_hub_download if available.

def load_config(model_key):
    path = CONFIG_DIR / f"{model_key}.yaml"
    if not path.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def try_hf_download(hf_id, dest_dir):
    if not HF_AVAILABLE:
        print("huggingface_hub not installed or not available.")
        return False
    token = os.environ.get("HUGGINGFACE_HUB_TOKEN")
    try:
        # We attempt to download all repo files by using list_repo_files (not available in all versions).
        # Simpler strategy: try to download the common artifacts names from config required_files.
        from huggingface_hub import list_repo_files
        files = list_repo_files(hf_id, token=token)
        print("HF files:", files[:6], "...")
        for fname in files:
            # skip large LFS pointers if necessary; here we will attempt download if it's sensible
            try:
                target = hf_hub_download(repo_id=hf_id, filename=fname, use_auth_token=token, repo_type="model")
                # move to dest
                target_path = Path(target)
                relative = Path(fname)
                dest_f = dest_dir / relative
                dest_f.parent.mkdir(parents=True, exist_ok=True)
                shutil.move(str(target_path), str(dest_f))
                print("Downloaded HF:", fname)
            except Exception as e:
                print("Warning, failed to download file:", fname, ":", e)
        return True
    except Exception as e:
        # fallback: try to download a small set of typical filenames
        print("HF download attempt failed:", e)
        return False

def try_git_clone(url, dest_dir):
    print("Attempting git clone:", url)
    try:
        subprocess.check_call(["git", "clone", "--depth", "1", url, str(dest_dir)])
        return True
    except Exception as e:
        print("git clone failed:", e)
        # cleanup partial clone
        if dest_dir.exists():
            shutil.rmtree(dest_dir)
        return False

def try_curl_download(url, dest_file):
    print("curl/request download:", url)
    try:
        r = requests.get(url, stream=True, timeout=30)
        r.raise_for_status()
        dest_file.parent.mkdir(parents=True, exist_ok=True)
        with open(dest_file, "wb") as f:
            for chunk in r.iter_content(1024*1024):
                f.write(chunk)
        return True
    except Exception as e:
        print("curl download failed:", e)
        return False

def ensure_required_files(dest_dir, required_list):
    ok = True
    missing = []
    for pat in required_list:
        # support wildcard-ish patterns like "vocoder/*"
        if pat.endswith("/*"):
            if not any(dest_dir.glob(pat.replace("/*", "/*"))):
                missing.append(pat)
                ok = False
        else:
            if not (dest_dir / pat).exists():
                missing.append(pat)
                ok = False
    return ok, missing

def download_model(model_key, prefer_sources=None):
    cfg = load_config(model_key)
    model_dir = MODELS_DIR / cfg["model_id"]
    model_dir.mkdir(parents=True, exist_ok=True)

    # Try sources in order: prefer_sources (if provided) -> yaml preferred_source
    sources = prefer_sources or cfg.get("preferred_source", [])
    print("Sources to try:", sources)

    succeeded = False
    for src in sources:
        if src.startswith("hf:") or src.startswith("hf "):
            hf_id = src.split(":", 1)[1] if ":" in src else src.split(None,1)[1]
            print("Trying Hugging Face:", hf_id)
            ok = try_hf_download(hf_id, model_dir)
            if ok:
                succeeded = True
                break
        elif src.startswith("modelscope:"):
            ms_key = src
            url = DEFAULT_SOURCE_MAP.get(ms_key)
            if url:
                ok = try_git_clone(url, model_dir)
                if ok:
                    succeeded = True
                    break
                else:
                    print("ModelScope git clone fallback failed for", ms_key)
            else:
                print("No mapping for ModelScope entry:", ms_key, " ‚Äî please add to DEFAULT_SOURCE_MAP.")
        elif src.startswith("http"):
            # direct URL to a tar or zip or git; if endswith .git try git clone:
            if src.endswith(".git"):
                if try_git_clone(src, model_dir):
                    succeeded = True
                    break
            else:
                # try direct download to model_dir/<basename>
                fname = Path(src).name
                if try_curl_download(src, model_dir / fname):
                    succeeded = True
                    break
    # final fallback: try hf id guessed from model_key
    if not succeeded and HF_AVAILABLE:
        guess = model_key
        try:
            print("Final HF guess attempt on:", guess)
            if try_hf_download(guess, model_dir):
                succeeded = True
        except Exception:
            pass

    if not succeeded:
        print(f"Failed to download {model_key} from listed sources. You may need to provide an internal mirror URL in DEFAULT_SOURCE_MAP or place files manually at {model_dir}.")
        return False, model_dir

    # verify required files
    required = cfg.get("required_files", [])
    ok, missing = ensure_required_files(model_dir, required)
    if not ok:
        print("WARNING: Required files missing after download:", missing)
        print("Please check the model folder or add the missing files manually.")
    else:
        print(f"{model_key} downloaded and required files present.")
    # update registry
    registry = {}
    if REGISTRY_FILE.exists():
        registry = json.loads(REGISTRY_FILE.read_text(encoding="utf-8"))
    registry[cfg["model_id"]] = {
        "path": str(model_dir),
        "display_name": cfg.get("display_name"),
        "size": cfg.get("size"),
        "sample_rate": cfg.get("sample_rate"),
        "preferred_vocoder": cfg.get("preferred_vocoder"),
    }
    REGISTRY_FILE.parent.mkdir(parents=True, exist_ok=True)
    REGISTRY_FILE.write_text(json.dumps(registry, indent=2), encoding="utf-8")
    return True, model_dir

def list_available_configs():
    for f in CONFIG_DIR.glob("*.yaml"):
        print("-", f.stem)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, help="model config name (filename without ext)")
    parser.add_argument("--all", action="store_true", help="download all configs")
    parser.add_argument("--prefer", type=str, nargs="*", help="override preferred sources (e.g. hf:index/foo modelscope:...)")
    args = parser.parse_args()

    if args.all:
        names = [p.stem for p in CONFIG_DIR.glob("*.yaml")]
    elif args.model:
        names = [args.model]
    else:
        print("Available model configs:")
        list_available_configs()
        sys.exit(0)

    for name in names:
        print("\n=== Downloading:", name)
        ok, path = download_model(name, prefer_sources=args.prefer)
        print("Result:", ok, path)

if __name__ == "__main__":
    main()
```

**Notes about the script**

- Edit `DEFAULT_SOURCE_MAP` to point to your company‚Äôs ModelScope mirror URLs or internal Git server if ModelScope is inaccessible.
    
- The script attempts to download all files listed in the HF repo via `list_repo_files` (requires a recent `huggingface_hub`). If your HF SDK is older or HF is blocked, it will fall back to `git clone` of ModelScope clones.
    
- If ModelScope requires auth, you‚Äôll need to either:
    
    - add `MODELSCOPE_TOKEN` as an environment variable and extend the script to use it in `git clone` (e.g., `https://<token>@modelscope.cn/...`), or
        
    - add internal mirror URLs into `DEFAULT_SOURCE_MAP`.
        

---

# 3) Simple wrapper scripts

`download_windows.ps1` (PowerShell)

```powershell
# run from project root
python .\scripts\download_models.py --all
if ($LASTEXITCODE -ne 0) { Write-Host "Download failed"; exit $LASTEXITCODE }
Write-Host "Downloads finished. Check models/registry.json"
```

`download_mac.sh` (bash)

```bash
#!/usr/bin/env bash
python3 scripts/download_models.py --all
if [ $? -ne 0 ]; then
  echo "Download failed"
  exit 1
fi
echo "Downloads finished. Check models/registry.json"
```

Mark `download_mac.sh` executable: `chmod +x download_mac.sh`.

---

# 4) Example: How to add an internal ModelScope/Git mirror (edit DEFAULT_SOURCE_MAP)

If your company blocks both HF and external ModelScope domain, point `DEFAULT_SOURCE_MAP` to your internal artifacts server (Nexus, Artifactory, internal Git). Example:

```python
DEFAULT_SOURCE_MAP["modelscope:iic/speech_cosyvoice2-300m"] = "https://git.internal.corp/models/speech_cosyvoice2-300m.git"
```

Then running `python scripts/download_models.py --model cosyvoice2-300m` will `git clone` from your internal URL.

---

# 5) Tokens, proxies, and networking tips

- **Hugging Face:** set `HUGGINGFACE_HUB_TOKEN` env var for private models. HF Python client uses `use_auth_token=True` automatically if token set.
    
- **ModelScope:** many hosted models are public, but some require login. If your company network requires a proxy, set `HTTP_PROXY`/`HTTPS_PROXY` env vars before running the script.
    
- **If both are blocked:** the script supports adding direct `http` URLs for tarballs in `preferred_source` (edit YAML to add `http://.../cosyvoice2.tar.gz`) or adding internal git URLs to `DEFAULT_SOURCE_MAP`.
    

---

# 6) Export helper stubs (optional automation)

If you want, I can add automation into `download_models.py` to automatically run:

- ONNX export for vocoders on Windows
    
- TorchScript export for Mac MPS  
    after successful download (using the `export_onnx_windows.py` / `export_torchscript_mac.py` we discussed before).
    

---

# 7) Quick usage / checklist

1. Put YAML configs into `models/configs/` (I provided the content above).
    
2. Edit `DEFAULT_SOURCE_MAP` in `scripts/download_models.py` to include your ModelScope or internal mirror clone URLs (one-time).
    
3. (Optional) Ensure `git` is installed (for `git clone` fallback).
    
4. Set environment tokens if model is private:
    
    - `export HUGGINGFACE_HUB_TOKEN=hf_xxx` (Linux/Mac) or set in PowerShell on Windows.
        
5. Run:
    
    - mac: `./download_mac.sh`
        
    - win: `.\download_windows.ps1`
        
    - or `python scripts/download_models.py --model cosyvoice2-300m`
        

---

If you want, next I can:

- add **automatic ONNX / TorchScript export** to run after download, or
    
- convert the YAML configs into a single `models.yml` registry and provide a small web UI to manage model installs,
    
- or tailor the `DEFAULT_SOURCE_MAP` entries for the **exact** ModelScope repo names you want me to embed (tell me the exact model IDs you prefer and I‚Äôll pre-fill the mapping).
    

Perfect ‚Äî we can do a **single `models.yml` registry** and a **small web UI** to:

- List models
    
- Show recommended device (CPU / MPS)
    
- Show sample rate, size, vocoder
    
- Download / update / verify files from HF or ModelScope
    
- Track which models are installed locally
    

I‚Äôll give a **ready-to-run solution** using **FastAPI + simple HTML + JS** (runs in browser on localhost). Works on Windows and Mac.

---

# 1Ô∏è‚É£ Unified `models.yml` registry

Save as `models/models.yml`:

```yaml
models:
  - id: cosyvoice2-300m
    display_name: CosyVoice2 300M
    type: tts
    architecture: cosyvoice2
    size: 300M
    sample_rate: 24000
    vocoder: bigvgan
    preferred_sources:
      - modelscope:iic/speech_cosyvoice2-300m
      - hf:iic/speech_cosyvoice2-300m
    recommended_device:
      windows_cpu: cpu
      mac_mps: mps
    required_files:
      - model.ckpt
      - config.json
      - tokenizer.json
      - vocoder/*
  - id: cosyvoice2-1.8b
    display_name: CosyVoice2 1.8B
    type: tts
    architecture: cosyvoice2
    size: 1.8B
    sample_rate: 24000
    vocoder: bigvgan
    preferred_sources:
      - modelscope:iic/speech_cosyvoice2-1.8b
      - hf:iic/speech_cosyvoice2-1.8b
    recommended_device:
      windows_cpu: cpu
      mac_mps: mps
    required_files:
      - model.ckpt
      - config.json
      - tokenizer.json
      - vocoder/*
  - id: indextts2_multilingual
    display_name: IndexTTS2 Multilingual
    type: tts-voice-clone
    architecture: indextts2
    size: ~300M
    sample_rate: 22050
    vocoder: hifigan
    preferred_sources:
      - modelscope:index/indextts2_multilingual
      - hf:index/indextts2_multilingual
    recommended_device:
      windows_cpu: cpu
      mac_mps: mps
    required_files:
      - model.pt
      - config.json
      - speaker_encoder.pth
      - vocoder/*
  - id: hifigan
    display_name: HiFi-GAN v1
    type: vocoder
    architecture: hifigan
    size: small
    sample_rate: 22050
    preferred_sources:
      - modelscope:repo/hifigan
      - hf:vits/hifigan
    recommended_device:
      windows_cpu: cpu
      mac_mps: mps
    required_files:
      - generator.pth
      - config_vocoder.json
  - id: bigvgan
    display_name: BigVGAN v2 Base
    type: vocoder
    architecture: bigvgan
    size: larger
    sample_rate: 24000
    preferred_sources:
      - modelscope:repo/bigvgan
      - hf:bigvgan/bigvgan-base
    recommended_device:
      windows_cpu: cpu
      mac_mps: mps
    required_files:
      - bigvgan.pth
      - config_vocoder.json
```

---

# 2Ô∏è‚É£ FastAPI Web UI

Create `scripts/web_ui.py`:

```python
import os
import yaml
import subprocess
from pathlib import Path
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from jinja2 import Template

ROOT = Path(__file__).resolve().parent.parent
MODELS_YAML = ROOT / "models" / "models.yml"
MODELS_DIR = ROOT / "models"

app = FastAPI()

# Mount static files (optional, for JS/CSS)
app.mount("/static", StaticFiles(directory=ROOT / "scripts" / "static"), name="static")

# Load models
with open(MODELS_YAML, "r", encoding="utf-8") as f:
    models_registry = yaml.safe_load(f)["models"]

HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Offline TTS Model Manager</title>
    <style>
        body { font-family: Arial; margin: 20px; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ccc; padding: 8px; text-align: left; }
        th { background-color: #eee; }
        button { padding: 5px 10px; }
        .installed { color: green; font-weight: bold; }
        .missing { color: red; font-weight: bold; }
    </style>
</head>
<body>
    <h1>Offline TTS Model Manager</h1>
    <table>
        <tr>
            <th>Model</th><th>Type</th><th>Size</th><th>Sample Rate</th><th>Vocoder</th><th>Device</th><th>Status</th><th>Action</th>
        </tr>
        {% for m in models %}
        <tr>
            <td>{{ m.display_name }}</td>
            <td>{{ m.type }}</td>
            <td>{{ m.size }}</td>
            <td>{{ m.sample_rate }}</td>
            <td>{{ m.vocoder or '-' }}</td>
            <td>{{ m.recommended_device }}</td>
            <td>
                {% if m.installed %}
                    <span class="installed">Installed</span>
                {% else %}
                    <span class="missing">Missing</span>
                {% endif %}
            </td>
            <td>
                {% if not m.installed %}
                    <form method="post" action="/download">
                        <input type="hidden" name="model_id" value="{{ m.id }}">
                        <button type="submit">Download</button>
                    </form>
                {% else %}
                    <button disabled>Download</button>
                {% endif %}
            </td>
        </tr>
        {% endfor %}
    </table>
</body>
</html>
"""

def check_installed(model_id, required_files):
    model_path = MODELS_DIR / model_id
    for f in required_files:
        if f.endswith("/*"):
            if not any(model_path.glob(f.replace("/*", "/*"))):
                return False
        else:
            if not (model_path / f).exists():
                return False
    return True

@app.get("/", response_class=HTMLResponse)
async def home():
    models_list = []
    for m in models_registry:
        installed = check_installed(m["id"], m.get("required_files", []))
        m_copy = m.copy()
        m_copy["installed"] = installed
        models_list.append(m_copy)
    template = Template(HTML_TEMPLATE)
    return template.render(models=models_list)

@app.post("/download")
async def download(request: Request):
    form = await request.form()
    model_id = form.get("model_id")
    if model_id:
        # Call the download_models.py script
        subprocess.Popen(["python", str(ROOT / "scripts" / "download_models.py"), "--model", model_id])
    return HTMLResponse("<html><body>Download started. <a href='/'>Back</a></body></html>")
```

---

# 3Ô∏è‚É£ Instructions

1. Save `models/models.yml` (from step 1)
    
2. Save `scripts/web_ui.py`
    
3. Ensure `download_models.py` is in `scripts/`
    
4. Run FastAPI:
    

```bash
pip install fastapi uvicorn jinja2 pyyaml
uvicorn scripts.web_ui:app --reload
```

5. Open browser at `http://127.0.0.1:8000`
    
6. The table shows all models, device recommendations, status, and a **Download** button for missing models.
    
7. Clicking **Download** runs `download_models.py` in the background.
    

---

# 4Ô∏è‚É£ Notes / Features

- **Cross-platform detection** is already in `download_models.py`
    
- **Installed check** verifies required files per model
    
- Can expand UI to **show progress / logs** (using WebSocket)
    
- Can **add search/filter** for vocoder, type, or device
    
- Models from HF / ModelScope / internal mirror supported
    
- **Single `models.yml` registry** simplifies adding new models ‚Äî just append a new entry
    

---

I can make a **next version** that adds:

- Live **download progress** on the page
    
- ONNX/TorchScript **auto-export after download**
    
- **One-click ‚Äúinstall all‚Äù** button
    
- Optional **proxy / token configuration page**
    

Do you want me to prepare that **enhanced web UI version** next?