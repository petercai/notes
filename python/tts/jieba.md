`jieba` (ç»“å·´åˆ†è¯) is one of the most popular **Chinese text segmentation** libraries in Python.

Because written Chinese doesnâ€™t use spaces between words, you need a tokenizer like `jieba` to split sentences into meaningful words â€” just like English spaces.

---

## ğŸ§  What Jieba Does

**Jieba = Chinese word segmentation + POS tagging + keyword extraction**

It can:

1. **Split** a sentence into words (`åˆ†è¯`)
    
2. **Tag** each word with its part of speech (e.g. noun, verb)
    
3. **Extract** top keywords using TF-IDF or TextRank
    
4. **Support custom dictionaries**
    

---

## âš™ï¸ Installation

```bash
pip install jieba
```

---

## ğŸ§© Basic Usage

### 1. **Accurate Mode** (default)

Splits text as precisely as possible.

```python
import jieba

text = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦"
words = jieba.cut(text)
print("/".join(words))
# Output: æˆ‘/æ¥åˆ°/åŒ—äº¬/æ¸…åå¤§å­¦
```

---

### 2. **Full Mode**

Cuts all possible words â€” faster but less accurate.

```python
jieba.cut(text, cut_all=True)
# Output: æˆ‘/æ¥åˆ°/åŒ—äº¬/æ¸…å/æ¸…åå¤§å­¦/åå¤§/å¤§å­¦
```

---

### 3. **Search Engine Mode**

For indexing â€” gives finer granularity.

```python
jieba.cut_for_search(text)
# Output: æˆ‘/æ¥åˆ°/åŒ—äº¬/æ¸…å/åå¤§/å¤§å­¦/æ¸…åå¤§å­¦
```

---

## ğŸ·ï¸ Part-of-Speech Tagging

```python
import jieba.posseg as pseg

words = pseg.cut("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†")
for w, flag in words:
    print(f"{w} ({flag})")

# Example:
# æˆ‘ (r)
# çˆ± (v)
# è‡ªç„¶è¯­è¨€ (n)
# å¤„ç† (v)
```

- `r` = pronoun
    
- `v` = verb
    
- `n` = noun
    

---

## ğŸ” Keyword Extraction

### TF-IDF

```python
import jieba.analyse

text = "å°æ˜ç¡•å£«æ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€ï¼Œååœ¨æ—¥æœ¬äº¬éƒ½å¤§å­¦æ·±é€ "
keywords = jieba.analyse.extract_tags(text, topK=3)
print(keywords)
# ['äº¬éƒ½å¤§å­¦', 'ä¸­å›½ç§‘å­¦é™¢', 'è®¡ç®—æ‰€']
```

### TextRank

```python
jieba.analyse.textrank(text, topK=3)
```

---

## ğŸ§° Custom Dictionary

You can add your own domain-specific words:

```python
jieba.add_word("è‡ªç„¶è¯­è¨€å¤„ç†")
jieba.add_word("å¤§æ¨¡å‹")
```

Or load a file:

```python
jieba.load_userdict("mydict.txt")
# each line: word freq POS_tag
# e.g. è‡ªç„¶è¯­è¨€å¤„ç† 10 n
```

---

## ğŸ§  Summary Table

|Feature|Function|
|---|---|
|Word segmentation|`jieba.cut(text)`|
|Full mode|`jieba.cut(text, cut_all=True)`|
|Search mode|`jieba.cut_for_search(text)`|
|POS tagging|`pseg.cut(text)`|
|Keyword extraction|`jieba.analyse.extract_tags(text)`|
|Custom dictionary|`jieba.add_word()` / `jieba.load_userdict()`|

---

Would you like me to show how `jieba` can integrate with `BeautifulSoup` â€” e.g. to extract and segment **Chinese text from HTML pages** automatically?