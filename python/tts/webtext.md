好，我们**重新开一个干净的话题**来把这个问题彻底讲清楚。

---

## 一、结论先行（一句话版）

**WeText 的 normalize 把中文“。”变成英文“.”，不是 Bug，而是刻意的“跨语言、跨模型统一标点规范”的设计**，目的是：

* 让 **TTS / ASR / TN / WFST / tokenizer** 的行为更稳定
* 降低中文全角符号对 **英文/多语模型** 的不可控影响
* 把“语言相关差异”提前在 **文本标准化阶段** 消解掉

---

## 二、为什么一定要把中文“。” → 英文“.”？

### 1️⃣ 中文全角标点在底层模型中是“不稳定信号”

从模型角度看：

| 标点 | Unicode | 问题                             |
| -- | ------- | ------------------------------ |
| 。  | U+3002  | **非 ASCII，全角**                 |
| .  | U+002E  | **ASCII，几乎所有 tokenizer 都稳定支持** |

很多问题都来自这里：

* tokenizer 中 `。` **可能是单 token，也可能被拆**
* 有些模型 **没见过足够多的“。”**
* 在多语模型里，`。` 的统计远不如 `.` 稳定

👉 **normalize 的职责就是：消除这种不确定性**

---

### 2️⃣ WFST / pynini / TN 系统对 ASCII 更友好

WeText 本质上是一个 **TN（Text Normalization）系统**，核心技术栈是：

* pynini
* WFST
* rule-based rewrite

而这些系统：

* 对 ASCII 标点支持最好
* 对全角标点支持“能用但不优雅”
* 很多 rule 是写成：

  ```text
  "." | "!" | "?"
  ```

  而不是 `。！？`

👉 所以 **先统一成英文标点，后面的规则会简单 10 倍**

---

### 3️⃣ 对 TTS 来说，“句号”不是语言单位，而是 prosody hint

对 TTS 模型来说：

* **“。”和“.” 本质都是一句结束**
* 模型学的是：

  > sentence boundary → pause / pitch reset

而不是“这是中文句号还是英文句号”。

👉 **统一为 `.` 能保证 pause 行为一致**

---

## 三、WeText normalize 到底还换了哪些符号？

下面是 **WeText / CosyVoice / PaddleSpeech TN 中常见的 normalize 映射表（概念级，和实际代码高度一致）**

---

### ✅ 1️⃣ 中文标点 → 英文标点（最核心）

| 中文 | 英文 |
| -- | -- |
| 。  | .  |
| ，  | ,  |
| ？  | ?  |
| ！  | !  |
| ：  | :  |
| ；  | ;  |
| （  | (  |
| ）  | )  |
| 【  | [  |
| 】  | ]  |
| 《  | <  |
| 》  | >  |
| “  | "  |
| ”  | "  |
| ‘  | '  |
| ’  | '  |

👉 **目的：统一 token / WFST / pause 行为**

---

### ✅ 2️⃣ 全角符号 → 半角符号

| 全角  | 半角  |
| --- | --- |
| ＡＢＣ | ABC |
| １２３ | 123 |
| ＋   | +   |
| －   | -   |
| ＝   | =   |
| ／   | /   |

👉 **这是“字符宽度归一化”**

---

### ✅ 3️⃣ 各种“点号” → 英文句点 `.`

| 原始字符 | 用途              |
| ---- | --------------- |
| ·    | 人名 / 分隔         |
| •    | 列表              |
| ‧    | 排版              |
| …    | 省略号（可能转成 `...`） |

👉 **为了避免 tokenizer 里出现“稀有点号”**

---

### ✅ 4️⃣ 破折号 / 连字符统一

| 原始 | 归一化 |
| -- | --- |
| —  | -   |
| –  | -   |
| ―  | -   |

---

### ✅ 5️⃣ 空白 & 不可见字符清理

* 全角空格 → 半角空格
* 多个空格 → 单个空格
* 去除：

  * zero-width space
  * NBSP
  * 奇怪的排版符

---

## 四、为什么 WeText 不是“只替换标点”这么简单？

因为 **WeText 的 normalize 是 TN pipeline 的第一道防线**：

```text
原始文本
 ↓
Unicode / 标点 / 宽度 normalize   ← 你问的就在这里
 ↓
数字 / 日期 / 时间 / 度量单位 TN
 ↓
语言特化 rewrite
 ↓
TTS / ASR / LM
```

如果不在第一步做这些：

* 后面每一条 rule 都要同时支持：

  * `。` 和 `.`
  * `，` 和 `,`
  * 全角 / 半角
* rule 数量直接 ×2～×5

👉 **工程上完全不可接受**

---

## 五、这会不会“伤害中文有声书的自然度”？

**不会。前提是：你在更高层正确分句。**

* WeText 只是把 `。` → `.`
* **真正影响自然度的是：**

  * 分句是否合理
  * 段落是否过长
  * 对话 / 旁白是否区分

这点你在 CosyVoice 项目里之前已经踩得很准了 👍

---

## 六、给你一个“有声书级”的建议（重要）

如果你是做 **中文小说 / audiobook**（你之前明确是）：

### ✅ 推荐做法

1. **保留中文原文用于存档 / 对齐**
2. **送入 TTS 前跑 WeText normalize**
3. **在 normalize 之后，再做你自己的“句长控制 & 对话感知切分”**

👉 不要试图“保留中文标点让模型自己理解”

---

## 七、如果你愿意，我可以下一步直接帮你做：

* ✅ **列出 WeText normalize 的真实代码级映射表**
* ✅ **给你一个「中文小说 → WeText 前后对比示例」**
* ✅ **帮你设计一个：保留中文标点信息但不干扰 TTS 的 pipeline**

你只需要告诉我：
👉 **你现在是偏 CosyVoice 还是 IndexTTS？**
